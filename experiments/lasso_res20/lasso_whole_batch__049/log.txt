Files already downloaded and verified
Files already downloaded and verified
DATALOADER FOR CLASS:  [0, 1, 2, 3, 4]
[ NETWORK ] [backbone] resnet20
[MODEL] Number of parameters :  275572
[ DEVICE  ] CUDA available
[LOSS FUNC] CrossEntropyLoss()  [OPTIMIZER] Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

====================== LOADING STATES... =====================
**[TEST] Loss 1.6464 (1.6668)	 Acc: 55.172 (54.600)	


==========================================================
#                                                          #
#                  Test acc : 54.6000                  #
#                                                          #
==========================================================

=====================================================================================
                                    Kernel Shape     Output Shape   Params  \
Layer                                                                        
0_conv1                            [3, 16, 3, 3]  [1, 16, 32, 32]    432.0   
1_bn1                                       [16]  [1, 16, 32, 32]     32.0   
2_layer1.0.Conv2d_conv1           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
3_layer1.0.BatchNorm2d_bn1                  [16]  [1, 16, 32, 32]     32.0   
4_layer1.0.Conv2d_conv2           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
5_layer1.0.BatchNorm2d_bn2                  [16]  [1, 16, 32, 32]     32.0   
6_layer1.0.Sequential_shortcut                 -  [1, 16, 32, 32]        -   
7_layer1.1.Conv2d_conv1           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
8_layer1.1.BatchNorm2d_bn1                  [16]  [1, 16, 32, 32]     32.0   
9_layer1.1.Conv2d_conv2           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
10_layer1.1.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
11_layer1.1.Sequential_shortcut                -  [1, 16, 32, 32]        -   
12_layer1.2.Conv2d_conv1          [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
13_layer1.2.BatchNorm2d_bn1                 [16]  [1, 16, 32, 32]     32.0   
14_layer1.2.Conv2d_conv2          [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
15_layer1.2.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
16_layer1.2.Sequential_shortcut                -  [1, 16, 32, 32]        -   
17_layer2.0.Conv2d_conv1          [16, 32, 3, 3]  [1, 32, 16, 16]   4.608k   
18_layer2.0.BatchNorm2d_bn1                 [32]  [1, 32, 16, 16]     64.0   
19_layer2.0.Conv2d_conv2          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
20_layer2.0.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
21_layer2.0.LambdaLayer_shortcut               -  [1, 32, 16, 16]        -   
22_layer2.1.Conv2d_conv1          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
23_layer2.1.BatchNorm2d_bn1                 [32]  [1, 32, 16, 16]     64.0   
24_layer2.1.Conv2d_conv2          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
25_layer2.1.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
26_layer2.1.Sequential_shortcut                -  [1, 32, 16, 16]        -   
27_layer2.2.Conv2d_conv1          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
28_layer2.2.BatchNorm2d_bn1                 [32]  [1, 32, 16, 16]     64.0   
29_layer2.2.Conv2d_conv2          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
30_layer2.2.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
31_layer2.2.Sequential_shortcut                -  [1, 32, 16, 16]        -   
32_layer3.0.Conv2d_conv1          [32, 64, 3, 3]    [1, 64, 8, 8]  18.432k   
33_layer3.0.BatchNorm2d_bn1                 [64]    [1, 64, 8, 8]    128.0   
34_layer3.0.Conv2d_conv2          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
35_layer3.0.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
36_layer3.0.LambdaLayer_shortcut               -    [1, 64, 8, 8]        -   
37_layer3.1.Conv2d_conv1          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
38_layer3.1.BatchNorm2d_bn1                 [64]    [1, 64, 8, 8]    128.0   
39_layer3.1.Conv2d_conv2          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
40_layer3.1.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
41_layer3.1.Sequential_shortcut                -    [1, 64, 8, 8]        -   
42_layer3.2.Conv2d_conv1          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
43_layer3.2.BatchNorm2d_bn1                 [64]    [1, 64, 8, 8]    128.0   
44_layer3.2.Conv2d_conv2          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
45_layer3.2.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
46_layer3.2.Sequential_shortcut                -    [1, 64, 8, 8]        -   
47_linear                              [64, 100]         [1, 100]     6.5k   

                                  Mult-Adds  
Layer                                        
0_conv1                            442.368k  
1_bn1                                  16.0  
2_layer1.0.Conv2d_conv1           2.359296M  
3_layer1.0.BatchNorm2d_bn1             16.0  
4_layer1.0.Conv2d_conv2           2.359296M  
5_layer1.0.BatchNorm2d_bn2             16.0  
6_layer1.0.Sequential_shortcut            -  
7_layer1.1.Conv2d_conv1           2.359296M  
8_layer1.1.BatchNorm2d_bn1             16.0  
9_layer1.1.Conv2d_conv2           2.359296M  
10_layer1.1.BatchNorm2d_bn2            16.0  
11_layer1.1.Sequential_shortcut           -  
12_layer1.2.Conv2d_conv1          2.359296M  
13_layer1.2.BatchNorm2d_bn1            16.0  
14_layer1.2.Conv2d_conv2          2.359296M  
15_layer1.2.BatchNorm2d_bn2            16.0  
16_layer1.2.Sequential_shortcut           -  
17_layer2.0.Conv2d_conv1          1.179648M  
18_layer2.0.BatchNorm2d_bn1            32.0  
19_layer2.0.Conv2d_conv2          2.359296M  
20_layer2.0.BatchNorm2d_bn2            32.0  
21_layer2.0.LambdaLayer_shortcut          -  
22_layer2.1.Conv2d_conv1          2.359296M  
23_layer2.1.BatchNorm2d_bn1            32.0  
24_layer2.1.Conv2d_conv2          2.359296M  
25_layer2.1.BatchNorm2d_bn2            32.0  
26_layer2.1.Sequential_shortcut           -  
27_layer2.2.Conv2d_conv1          2.359296M  
28_layer2.2.BatchNorm2d_bn1            32.0  
29_layer2.2.Conv2d_conv2          2.359296M  
30_layer2.2.BatchNorm2d_bn2            32.0  
31_layer2.2.Sequential_shortcut           -  
32_layer3.0.Conv2d_conv1          1.179648M  
33_layer3.0.BatchNorm2d_bn1            64.0  
34_layer3.0.Conv2d_conv2          2.359296M  
35_layer3.0.BatchNorm2d_bn2            64.0  
36_layer3.0.LambdaLayer_shortcut          -  
37_layer3.1.Conv2d_conv1          2.359296M  
38_layer3.1.BatchNorm2d_bn1            64.0  
39_layer3.1.Conv2d_conv2          2.359296M  
40_layer3.1.BatchNorm2d_bn2            64.0  
41_layer3.1.Sequential_shortcut           -  
42_layer3.2.Conv2d_conv1          2.359296M  
43_layer3.2.BatchNorm2d_bn1            64.0  
44_layer3.2.Conv2d_conv2          2.359296M  
45_layer3.2.BatchNorm2d_bn2            64.0  
46_layer3.2.Sequential_shortcut           -  
47_linear                              6.4k  
-------------------------------------------------------------------------------------
                          Totals
Total params            275.572k
Trainable params        275.572k
Non-trainable params         0.0
Mult-Adds             40.557488M
=====================================================================================



THIS IS LASSO COMPRESS!!!!!!!!!!!!!!!!



nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
nnnnnnmnmnmnmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmnmnmnmnnnnnn
num_pruned 9
num_pruned 9
num_pruned 9
num_pruned 17
num_pruned 17
num_pruned 17
num_pruned 33
num_pruned 33
num_pruned 33
[[], [], [7, 14, 10, 8, 5, 3, 2], [1, 15, 14, 12, 7, 0, 3], [15, 10, 11, 7, 6, 4, 13], [13, 20, 30, 28, 15, 26, 7, 21, 9, 10, 11, 14, 2, 0, 31], [1, 31, 28, 24, 20, 18, 17, 12, 10, 8, 6, 2, 3, 21, 19], [16, 31, 26, 23, 22, 18, 0, 12, 10, 8, 2, 6, 24, 25, 27], [63, 2, 1, 15, 6, 12, 5, 21, 24, 25, 33, 34, 37, 32, 62, 57, 53, 47, 46, 45, 43, 42, 41, 35, 17, 31, 60, 39, 44, 0, 61], [63, 12, 6, 4, 0, 9, 17, 19, 24, 28, 29, 38, 1, 61, 56, 55, 51, 48, 47, 45, 40, 30, 23, 3, 16, 22, 59, 36, 15, 5, 58], [60, 2, 59, 4, 21, 52, 51, 49, 48, 44, 26, 12, 35, 36, 38, 37, 14, 30, 41, 42, 58, 9, 7, 5, 45, 17, 8, 27, 25, 56, 32], []]
=====================================================================================
                                    Kernel Shape     Output Shape   Params  \
Layer                                                                        
0_conv1                            [3, 16, 3, 3]  [1, 16, 32, 32]    432.0   
1_bn1                                       [16]  [1, 16, 32, 32]     32.0   
2_layer1.0.Conv2d_conv1            [16, 7, 3, 3]   [1, 7, 32, 32]   1.008k   
3_layer1.0.BatchNorm2d_bn1                   [7]   [1, 7, 32, 32]     14.0   
4_layer1.0.Conv2d_conv2            [7, 16, 3, 3]  [1, 16, 32, 32]   1.008k   
5_layer1.0.BatchNorm2d_bn2                  [16]  [1, 16, 32, 32]     32.0   
6_layer1.0.Sequential_shortcut                 -  [1, 16, 32, 32]        -   
7_layer1.1.Conv2d_conv1            [16, 7, 3, 3]   [1, 7, 32, 32]   1.008k   
8_layer1.1.BatchNorm2d_bn1                   [7]   [1, 7, 32, 32]     14.0   
9_layer1.1.Conv2d_conv2            [7, 16, 3, 3]  [1, 16, 32, 32]   1.008k   
10_layer1.1.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
11_layer1.1.Sequential_shortcut                -  [1, 16, 32, 32]        -   
12_layer1.2.Conv2d_conv1           [16, 7, 3, 3]   [1, 7, 32, 32]   1.008k   
13_layer1.2.BatchNorm2d_bn1                  [7]   [1, 7, 32, 32]     14.0   
14_layer1.2.Conv2d_conv2           [7, 16, 3, 3]  [1, 16, 32, 32]   1.008k   
15_layer1.2.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
16_layer1.2.Sequential_shortcut                -  [1, 16, 32, 32]        -   
17_layer2.0.Conv2d_conv1          [16, 15, 3, 3]  [1, 15, 16, 16]    2.16k   
18_layer2.0.BatchNorm2d_bn1                 [15]  [1, 15, 16, 16]     30.0   
19_layer2.0.Conv2d_conv2          [15, 32, 3, 3]  [1, 32, 16, 16]    4.32k   
20_layer2.0.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
21_layer2.0.LambdaLayer_shortcut               -  [1, 32, 16, 16]        -   
22_layer2.1.Conv2d_conv1          [32, 15, 3, 3]  [1, 15, 16, 16]    4.32k   
23_layer2.1.BatchNorm2d_bn1                 [15]  [1, 15, 16, 16]     30.0   
24_layer2.1.Conv2d_conv2          [15, 32, 3, 3]  [1, 32, 16, 16]    4.32k   
25_layer2.1.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
26_layer2.1.Sequential_shortcut                -  [1, 32, 16, 16]        -   
27_layer2.2.Conv2d_conv1          [32, 15, 3, 3]  [1, 15, 16, 16]    4.32k   
28_layer2.2.BatchNorm2d_bn1                 [15]  [1, 15, 16, 16]     30.0   
29_layer2.2.Conv2d_conv2          [15, 32, 3, 3]  [1, 32, 16, 16]    4.32k   
30_layer2.2.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
31_layer2.2.Sequential_shortcut                -  [1, 32, 16, 16]        -   
32_layer3.0.Conv2d_conv1          [32, 31, 3, 3]    [1, 31, 8, 8]   8.928k   
33_layer3.0.BatchNorm2d_bn1                 [31]    [1, 31, 8, 8]     62.0   
34_layer3.0.Conv2d_conv2          [31, 64, 3, 3]    [1, 64, 8, 8]  17.856k   
35_layer3.0.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
36_layer3.0.LambdaLayer_shortcut               -    [1, 64, 8, 8]        -   
37_layer3.1.Conv2d_conv1          [64, 31, 3, 3]    [1, 31, 8, 8]  17.856k   
38_layer3.1.BatchNorm2d_bn1                 [31]    [1, 31, 8, 8]     62.0   
39_layer3.1.Conv2d_conv2          [31, 64, 3, 3]    [1, 64, 8, 8]  17.856k   
40_layer3.1.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
41_layer3.1.Sequential_shortcut                -    [1, 64, 8, 8]        -   
42_layer3.2.Conv2d_conv1          [64, 31, 3, 3]    [1, 31, 8, 8]  17.856k   
43_layer3.2.BatchNorm2d_bn1                 [31]    [1, 31, 8, 8]     62.0   
44_layer3.2.Conv2d_conv2          [31, 64, 3, 3]    [1, 64, 8, 8]  17.856k   
45_layer3.2.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
46_layer3.2.Sequential_shortcut                -    [1, 64, 8, 8]        -   
47_linear                              [64, 100]         [1, 100]     6.5k   

                                  Mult-Adds  
Layer                                        
0_conv1                            442.368k  
1_bn1                                  16.0  
2_layer1.0.Conv2d_conv1           1.032192M  
3_layer1.0.BatchNorm2d_bn1              7.0  
4_layer1.0.Conv2d_conv2           1.032192M  
5_layer1.0.BatchNorm2d_bn2             16.0  
6_layer1.0.Sequential_shortcut            -  
7_layer1.1.Conv2d_conv1           1.032192M  
8_layer1.1.BatchNorm2d_bn1              7.0  
9_layer1.1.Conv2d_conv2           1.032192M  
10_layer1.1.BatchNorm2d_bn2            16.0  
11_layer1.1.Sequential_shortcut           -  
12_layer1.2.Conv2d_conv1          1.032192M  
13_layer1.2.BatchNorm2d_bn1             7.0  
14_layer1.2.Conv2d_conv2          1.032192M  
15_layer1.2.BatchNorm2d_bn2            16.0  
16_layer1.2.Sequential_shortcut           -  
17_layer2.0.Conv2d_conv1            552.96k  
18_layer2.0.BatchNorm2d_bn1            15.0  
19_layer2.0.Conv2d_conv2           1.10592M  
20_layer2.0.BatchNorm2d_bn2            32.0  
21_layer2.0.LambdaLayer_shortcut          -  
22_layer2.1.Conv2d_conv1           1.10592M  
23_layer2.1.BatchNorm2d_bn1            15.0  
24_layer2.1.Conv2d_conv2           1.10592M  
25_layer2.1.BatchNorm2d_bn2            32.0  
26_layer2.1.Sequential_shortcut           -  
27_layer2.2.Conv2d_conv1           1.10592M  
28_layer2.2.BatchNorm2d_bn1            15.0  
29_layer2.2.Conv2d_conv2           1.10592M  
30_layer2.2.BatchNorm2d_bn2            32.0  
31_layer2.2.Sequential_shortcut           -  
32_layer3.0.Conv2d_conv1           571.392k  
33_layer3.0.BatchNorm2d_bn1            31.0  
34_layer3.0.Conv2d_conv2          1.142784M  
35_layer3.0.BatchNorm2d_bn2            64.0  
36_layer3.0.LambdaLayer_shortcut          -  
37_layer3.1.Conv2d_conv1          1.142784M  
38_layer3.1.BatchNorm2d_bn1            31.0  
39_layer3.1.Conv2d_conv2          1.142784M  
40_layer3.1.BatchNorm2d_bn2            64.0  
41_layer3.1.Sequential_shortcut           -  
42_layer3.2.Conv2d_conv1          1.142784M  
43_layer3.2.BatchNorm2d_bn1            31.0  
44_layer3.2.Conv2d_conv2          1.142784M  
45_layer3.2.BatchNorm2d_bn2            64.0  
46_layer3.2.Sequential_shortcut           -  
47_linear                              6.4k  
-------------------------------------------------------------------------------------
                          Totals
Total params             135.97k
Trainable params         135.97k
Non-trainable params         0.0
Mult-Adds             19.010303M
=====================================================================================
**[TEST] Loss 1.5735 (1.6936)	 Acc: 61.207 (55.200)	


==========================================================
#                                                          #
#                  Test acc : 55.2000                  #
#                                                          #
==========================================================

[Epoch 10/30] [TRAIN] Loss 1.2325 (1.4811)	 Acc: 75.000 (63.360)	 Time 0.026 (0.047)
**[Epoch 10/30] [VAL] Loss 1.0771 (1.3310)	 Acc: 66.176 (66.600)	 TotalTime: 0h 0min

[Epoch 20/30] [TRAIN] Loss 0.7000 (1.0751)	 Acc: 79.412 (70.960)	 Time 0.026 (0.046)
**[Epoch 20/30] [VAL] Loss 0.9759 (1.0328)	 Acc: 70.588 (71.280)	 TotalTime: 0h 0min

[Epoch 30/30] [TRAIN] Loss 1.1921 (0.9717)	 Acc: 67.647 (70.760)	 Time 0.027 (0.048)
**[Epoch 30/30] [VAL] Loss 1.2575 (1.0051)	 Acc: 63.235 (70.760)	 TotalTime: 0h 1min

**[TEST] Loss 1.0202 (0.9715)	 Acc: 69.828 (73.200)	


==========================================================
#                                                          #
#                  Test acc : 73.2000                  #
#                                                          #
==========================================================

