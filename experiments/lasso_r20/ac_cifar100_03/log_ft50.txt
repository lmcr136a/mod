Files already downloaded and verified
Files already downloaded and verified
DATALOADER FOR CLASS:  [0, 1, 2, 3, 4]
[ NETWORK ] [backbone] resnet20
[MODEL] Number of parameters :  275572
[ DEVICE  ] CUDA available
[LOSS FUNC] CrossEntropyLoss()  [OPTIMIZER] Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

====================== LOADING STATES... =====================
**[TEST] Loss 1.6464 (1.6668)	 Acc: 55.172 (54.600)	


==========================================================
#                                                          #
#                  Test acc : 54.6000                  #
#                                                          #
==========================================================

=====================================================================================
                                    Kernel Shape     Output Shape   Params  \
Layer                                                                        
0_conv1                            [3, 16, 3, 3]  [1, 16, 32, 32]    432.0   
1_bn1                                       [16]  [1, 16, 32, 32]     32.0   
2_layer1.0.Conv2d_conv1           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
3_layer1.0.BatchNorm2d_bn1                  [16]  [1, 16, 32, 32]     32.0   
4_layer1.0.Conv2d_conv2           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
5_layer1.0.BatchNorm2d_bn2                  [16]  [1, 16, 32, 32]     32.0   
6_layer1.0.Sequential_shortcut                 -  [1, 16, 32, 32]        -   
7_layer1.1.Conv2d_conv1           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
8_layer1.1.BatchNorm2d_bn1                  [16]  [1, 16, 32, 32]     32.0   
9_layer1.1.Conv2d_conv2           [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
10_layer1.1.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
11_layer1.1.Sequential_shortcut                -  [1, 16, 32, 32]        -   
12_layer1.2.Conv2d_conv1          [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
13_layer1.2.BatchNorm2d_bn1                 [16]  [1, 16, 32, 32]     32.0   
14_layer1.2.Conv2d_conv2          [16, 16, 3, 3]  [1, 16, 32, 32]   2.304k   
15_layer1.2.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
16_layer1.2.Sequential_shortcut                -  [1, 16, 32, 32]        -   
17_layer2.0.Conv2d_conv1          [16, 32, 3, 3]  [1, 32, 16, 16]   4.608k   
18_layer2.0.BatchNorm2d_bn1                 [32]  [1, 32, 16, 16]     64.0   
19_layer2.0.Conv2d_conv2          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
20_layer2.0.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
21_layer2.0.LambdaLayer_shortcut               -  [1, 32, 16, 16]        -   
22_layer2.1.Conv2d_conv1          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
23_layer2.1.BatchNorm2d_bn1                 [32]  [1, 32, 16, 16]     64.0   
24_layer2.1.Conv2d_conv2          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
25_layer2.1.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
26_layer2.1.Sequential_shortcut                -  [1, 32, 16, 16]        -   
27_layer2.2.Conv2d_conv1          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
28_layer2.2.BatchNorm2d_bn1                 [32]  [1, 32, 16, 16]     64.0   
29_layer2.2.Conv2d_conv2          [32, 32, 3, 3]  [1, 32, 16, 16]   9.216k   
30_layer2.2.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
31_layer2.2.Sequential_shortcut                -  [1, 32, 16, 16]        -   
32_layer3.0.Conv2d_conv1          [32, 64, 3, 3]    [1, 64, 8, 8]  18.432k   
33_layer3.0.BatchNorm2d_bn1                 [64]    [1, 64, 8, 8]    128.0   
34_layer3.0.Conv2d_conv2          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
35_layer3.0.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
36_layer3.0.LambdaLayer_shortcut               -    [1, 64, 8, 8]        -   
37_layer3.1.Conv2d_conv1          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
38_layer3.1.BatchNorm2d_bn1                 [64]    [1, 64, 8, 8]    128.0   
39_layer3.1.Conv2d_conv2          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
40_layer3.1.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
41_layer3.1.Sequential_shortcut                -    [1, 64, 8, 8]        -   
42_layer3.2.Conv2d_conv1          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
43_layer3.2.BatchNorm2d_bn1                 [64]    [1, 64, 8, 8]    128.0   
44_layer3.2.Conv2d_conv2          [64, 64, 3, 3]    [1, 64, 8, 8]  36.864k   
45_layer3.2.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
46_layer3.2.Sequential_shortcut                -    [1, 64, 8, 8]        -   
47_linear                              [64, 100]         [1, 100]     6.5k   

                                  Mult-Adds  
Layer                                        
0_conv1                            442.368k  
1_bn1                                  16.0  
2_layer1.0.Conv2d_conv1           2.359296M  
3_layer1.0.BatchNorm2d_bn1             16.0  
4_layer1.0.Conv2d_conv2           2.359296M  
5_layer1.0.BatchNorm2d_bn2             16.0  
6_layer1.0.Sequential_shortcut            -  
7_layer1.1.Conv2d_conv1           2.359296M  
8_layer1.1.BatchNorm2d_bn1             16.0  
9_layer1.1.Conv2d_conv2           2.359296M  
10_layer1.1.BatchNorm2d_bn2            16.0  
11_layer1.1.Sequential_shortcut           -  
12_layer1.2.Conv2d_conv1          2.359296M  
13_layer1.2.BatchNorm2d_bn1            16.0  
14_layer1.2.Conv2d_conv2          2.359296M  
15_layer1.2.BatchNorm2d_bn2            16.0  
16_layer1.2.Sequential_shortcut           -  
17_layer2.0.Conv2d_conv1          1.179648M  
18_layer2.0.BatchNorm2d_bn1            32.0  
19_layer2.0.Conv2d_conv2          2.359296M  
20_layer2.0.BatchNorm2d_bn2            32.0  
21_layer2.0.LambdaLayer_shortcut          -  
22_layer2.1.Conv2d_conv1          2.359296M  
23_layer2.1.BatchNorm2d_bn1            32.0  
24_layer2.1.Conv2d_conv2          2.359296M  
25_layer2.1.BatchNorm2d_bn2            32.0  
26_layer2.1.Sequential_shortcut           -  
27_layer2.2.Conv2d_conv1          2.359296M  
28_layer2.2.BatchNorm2d_bn1            32.0  
29_layer2.2.Conv2d_conv2          2.359296M  
30_layer2.2.BatchNorm2d_bn2            32.0  
31_layer2.2.Sequential_shortcut           -  
32_layer3.0.Conv2d_conv1          1.179648M  
33_layer3.0.BatchNorm2d_bn1            64.0  
34_layer3.0.Conv2d_conv2          2.359296M  
35_layer3.0.BatchNorm2d_bn2            64.0  
36_layer3.0.LambdaLayer_shortcut          -  
37_layer3.1.Conv2d_conv1          2.359296M  
38_layer3.1.BatchNorm2d_bn1            64.0  
39_layer3.1.Conv2d_conv2          2.359296M  
40_layer3.1.BatchNorm2d_bn2            64.0  
41_layer3.1.Sequential_shortcut           -  
42_layer3.2.Conv2d_conv1          2.359296M  
43_layer3.2.BatchNorm2d_bn1            64.0  
44_layer3.2.Conv2d_conv2          2.359296M  
45_layer3.2.BatchNorm2d_bn2            64.0  
46_layer3.2.Sequential_shortcut           -  
47_linear                              6.4k  
-------------------------------------------------------------------------------------
                          Totals
Total params            275.572k
Trainable params        275.572k
Non-trainable params         0.0
Mult-Adds             40.557488M
=====================================================================================
Files already downloaded and verified
Files already downloaded and verified
DATALOADER FOR ALL CLASS
DATALOADER LABELS:  [19, 29, 0, 11, 1, 86, 90, 28, 23, 31, 39, 96, 82, 17, 71, 39, 8, 97, 80, 71, 74, 59, 70, 87, 59, 84, 64, 52, 42, 64]
mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
BasicBlock(
  (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 12
BasicBlock(
  (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 12
BasicBlock(
  (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 12
BasicBlock(
  (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): LambdaLayer()
)
num_pruned 23
BasicBlock(
  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 23
BasicBlock(
  (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 23
BasicBlock(
  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): LambdaLayer()
)
num_pruned 45
BasicBlock(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 45
BasicBlock(
  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (shortcut): Sequential()
)
num_pruned 45
Linear(in_features=64, out_features=100, bias=True)
[[], [], [[3, 5, 7, 8]], [[1, 3, 12, 14]], [[7, 10, 11, 13]], [[7, 9, 10, 15, 18, 20, 21, 30, 31]], [[1, 6, 8, 10, 12, 17, 20, 27, 31]], [[0, 2, 8, 12, 16, 18, 25, 27, 31]], [[0, 8, 13, 15, 24, 32, 33, 37, 39, 41, 42, 43, 46, 47, 55, 56, 58, 60, 62]], [[0, 2, 4, 5, 9, 14, 16, 17, 19, 22, 32, 37, 42, 47, 48, 50, 56, 61, 63]], [[1, 2, 4, 5, 7, 12, 14, 18, 19, 26, 27, 35, 36, 37, 41, 45, 48, 49, 51]], []]
=====================================================================================
                                    Kernel Shape     Output Shape   Params  \
Layer                                                                        
0_conv1                            [3, 16, 3, 3]  [1, 16, 32, 32]    432.0   
1_bn1                                       [16]  [1, 16, 32, 32]     32.0   
2_layer1.0.Conv2d_conv1            [16, 4, 3, 3]   [1, 4, 32, 32]    576.0   
3_layer1.0.BatchNorm2d_bn1                   [4]   [1, 4, 32, 32]      8.0   
4_layer1.0.Conv2d_conv2            [4, 16, 3, 3]  [1, 16, 32, 32]    576.0   
5_layer1.0.BatchNorm2d_bn2                  [16]  [1, 16, 32, 32]     32.0   
6_layer1.0.Sequential_shortcut                 -  [1, 16, 32, 32]        -   
7_layer1.1.Conv2d_conv1            [16, 4, 3, 3]   [1, 4, 32, 32]    576.0   
8_layer1.1.BatchNorm2d_bn1                   [4]   [1, 4, 32, 32]      8.0   
9_layer1.1.Conv2d_conv2            [4, 16, 3, 3]  [1, 16, 32, 32]    576.0   
10_layer1.1.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
11_layer1.1.Sequential_shortcut                -  [1, 16, 32, 32]        -   
12_layer1.2.Conv2d_conv1           [16, 4, 3, 3]   [1, 4, 32, 32]    576.0   
13_layer1.2.BatchNorm2d_bn1                  [4]   [1, 4, 32, 32]      8.0   
14_layer1.2.Conv2d_conv2           [4, 16, 3, 3]  [1, 16, 32, 32]    576.0   
15_layer1.2.BatchNorm2d_bn2                 [16]  [1, 16, 32, 32]     32.0   
16_layer1.2.Sequential_shortcut                -  [1, 16, 32, 32]        -   
17_layer2.0.Conv2d_conv1           [16, 9, 3, 3]   [1, 9, 16, 16]   1.296k   
18_layer2.0.BatchNorm2d_bn1                  [9]   [1, 9, 16, 16]     18.0   
19_layer2.0.Conv2d_conv2           [9, 32, 3, 3]  [1, 32, 16, 16]   2.592k   
20_layer2.0.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
21_layer2.0.LambdaLayer_shortcut               -  [1, 32, 16, 16]        -   
22_layer2.1.Conv2d_conv1           [32, 9, 3, 3]   [1, 9, 16, 16]   2.592k   
23_layer2.1.BatchNorm2d_bn1                  [9]   [1, 9, 16, 16]     18.0   
24_layer2.1.Conv2d_conv2           [9, 32, 3, 3]  [1, 32, 16, 16]   2.592k   
25_layer2.1.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
26_layer2.1.Sequential_shortcut                -  [1, 32, 16, 16]        -   
27_layer2.2.Conv2d_conv1           [32, 9, 3, 3]   [1, 9, 16, 16]   2.592k   
28_layer2.2.BatchNorm2d_bn1                  [9]   [1, 9, 16, 16]     18.0   
29_layer2.2.Conv2d_conv2           [9, 32, 3, 3]  [1, 32, 16, 16]   2.592k   
30_layer2.2.BatchNorm2d_bn2                 [32]  [1, 32, 16, 16]     64.0   
31_layer2.2.Sequential_shortcut                -  [1, 32, 16, 16]        -   
32_layer3.0.Conv2d_conv1          [32, 19, 3, 3]    [1, 19, 8, 8]   5.472k   
33_layer3.0.BatchNorm2d_bn1                 [19]    [1, 19, 8, 8]     38.0   
34_layer3.0.Conv2d_conv2          [19, 64, 3, 3]    [1, 64, 8, 8]  10.944k   
35_layer3.0.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
36_layer3.0.LambdaLayer_shortcut               -    [1, 64, 8, 8]        -   
37_layer3.1.Conv2d_conv1          [64, 19, 3, 3]    [1, 19, 8, 8]  10.944k   
38_layer3.1.BatchNorm2d_bn1                 [19]    [1, 19, 8, 8]     38.0   
39_layer3.1.Conv2d_conv2          [19, 64, 3, 3]    [1, 64, 8, 8]  10.944k   
40_layer3.1.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
41_layer3.1.Sequential_shortcut                -    [1, 64, 8, 8]        -   
42_layer3.2.Conv2d_conv1          [64, 19, 3, 3]    [1, 19, 8, 8]  10.944k   
43_layer3.2.BatchNorm2d_bn1                 [19]    [1, 19, 8, 8]     38.0   
44_layer3.2.Conv2d_conv2          [19, 64, 3, 3]    [1, 64, 8, 8]  10.944k   
45_layer3.2.BatchNorm2d_bn2                 [64]    [1, 64, 8, 8]    128.0   
46_layer3.2.Sequential_shortcut                -    [1, 64, 8, 8]        -   
47_linear                              [64, 100]         [1, 100]     6.5k   

                                 Mult-Adds  
Layer                                       
0_conv1                           442.368k  
1_bn1                                 16.0  
2_layer1.0.Conv2d_conv1           589.824k  
3_layer1.0.BatchNorm2d_bn1             4.0  
4_layer1.0.Conv2d_conv2           589.824k  
5_layer1.0.BatchNorm2d_bn2            16.0  
6_layer1.0.Sequential_shortcut           -  
7_layer1.1.Conv2d_conv1           589.824k  
8_layer1.1.BatchNorm2d_bn1             4.0  
9_layer1.1.Conv2d_conv2           589.824k  
10_layer1.1.BatchNorm2d_bn2           16.0  
11_layer1.1.Sequential_shortcut          -  
12_layer1.2.Conv2d_conv1          589.824k  
13_layer1.2.BatchNorm2d_bn1            4.0  
14_layer1.2.Conv2d_conv2          589.824k  
15_layer1.2.BatchNorm2d_bn2           16.0  
16_layer1.2.Sequential_shortcut          -  
17_layer2.0.Conv2d_conv1          331.776k  
18_layer2.0.BatchNorm2d_bn1            9.0  
19_layer2.0.Conv2d_conv2          663.552k  
20_layer2.0.BatchNorm2d_bn2           32.0  
21_layer2.0.LambdaLayer_shortcut         -  
22_layer2.1.Conv2d_conv1          663.552k  
23_layer2.1.BatchNorm2d_bn1            9.0  
24_layer2.1.Conv2d_conv2          663.552k  
25_layer2.1.BatchNorm2d_bn2           32.0  
26_layer2.1.Sequential_shortcut          -  
27_layer2.2.Conv2d_conv1          663.552k  
28_layer2.2.BatchNorm2d_bn1            9.0  
29_layer2.2.Conv2d_conv2          663.552k  
30_layer2.2.BatchNorm2d_bn2           32.0  
31_layer2.2.Sequential_shortcut          -  
32_layer3.0.Conv2d_conv1          350.208k  
33_layer3.0.BatchNorm2d_bn1           19.0  
34_layer3.0.Conv2d_conv2          700.416k  
35_layer3.0.BatchNorm2d_bn2           64.0  
36_layer3.0.LambdaLayer_shortcut         -  
37_layer3.1.Conv2d_conv1          700.416k  
38_layer3.1.BatchNorm2d_bn1           19.0  
39_layer3.1.Conv2d_conv2          700.416k  
40_layer3.1.BatchNorm2d_bn2           64.0  
41_layer3.1.Sequential_shortcut          -  
42_layer3.2.Conv2d_conv1          700.416k  
43_layer3.2.BatchNorm2d_bn1           19.0  
44_layer3.2.Conv2d_conv2          700.416k  
45_layer3.2.BatchNorm2d_bn2           64.0  
46_layer3.2.Sequential_shortcut          -  
47_linear                             6.4k  
-------------------------------------------------------------------------------------
                          Totals
Total params             85.732k
Trainable params         85.732k
Non-trainable params         0.0
Mult-Adds             11.489984M
=====================================================================================
**[TEST] Loss 4.8477 (4.8161)	 Acc: 5.172 (5.000)	


==========================================================
#                                                          #
#                  Test acc : 5.0000                  #
#                                                          #
==========================================================

[Epoch 10/50] [TRAIN] Loss 2.8064 (2.3723)	 Acc: 30.882 (43.360)	 Time 0.071 (0.101)
**[Epoch 10/50] [VAL] Loss 2.3442 (2.3600)	 Acc: 45.588 (42.000)	 TotalTime: 0h 0min

[Epoch 20/50] [TRAIN] Loss 1.4951 (1.8030)	 Acc: 51.471 (47.280)	 Time 0.052 (0.076)
**[Epoch 20/50] [VAL] Loss 1.8270 (1.7234)	 Acc: 47.059 (49.800)	 TotalTime: 0h 1min

[Epoch 30/50] [TRAIN] Loss 1.7509 (1.5803)	 Acc: 47.059 (51.080)	 Time 0.051 (0.082)
**[Epoch 30/50] [VAL] Loss 1.2743 (1.5606)	 Acc: 60.294 (51.760)	 TotalTime: 0h 1min

[Epoch 40/50] [TRAIN] Loss 1.4706 (1.3925)	 Acc: 51.471 (54.600)	 Time 0.029 (0.078)
**[Epoch 40/50] [VAL] Loss 1.6082 (1.4259)	 Acc: 45.588 (53.680)	 TotalTime: 0h 1min

[Epoch 50/50] [TRAIN] Loss 1.3350 (1.3537)	 Acc: 58.824 (55.360)	 Time 0.036 (0.088)
**[Epoch 50/50] [VAL] Loss 1.0235 (1.3101)	 Acc: 64.706 (56.440)	 TotalTime: 0h 2min

**[TEST] Loss 1.4991 (1.3277)	 Acc: 50.862 (54.000)	


==========================================================
#                                                          #
#                  Test acc : 54.0000                  #
#                                                          #
==========================================================

