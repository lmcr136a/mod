for_trainNval :     INV100
INV DATALOADER FOR CLASS:  [0, 1, 2, 3, 4]
for_test :     CIFAR100
Files already downloaded and verified
Files already downloaded and verified
CIFAR DATALOADER FOR CLASS:  [0, 1, 2, 3, 4]
[ NETWORK ]  resnet34
[MODEL] Number of parameters :  21328292
[ DEVICE  ] CUDA GPU 0 available
[LOSS FUNC] CrossEntropyLoss()  [OPTIMIZER] Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)

====================== LOADING STATES... =====================
**[TEST] Loss 1.0466)	 Acc: 73.000)	


==========================================================
#                                                          #
#                  Test acc : 73.0000                  #
#                                                          #
==========================================================

==============================================================================================
                                         Kernel Shape      Output Shape  \
Layer                                                                     
0_conv1                                 [3, 64, 3, 3]   [1, 64, 32, 32]   
1_bn1                                            [64]   [1, 64, 32, 32]   
2_layer1.0.Conv2d_conv1                [64, 64, 3, 3]   [1, 64, 32, 32]   
3_layer1.0.BatchNorm2d_bn1                       [64]   [1, 64, 32, 32]   
4_layer1.0.ReLU_relu1                               -   [1, 64, 32, 32]   
5_layer1.0.Conv2d_conv2                [64, 64, 3, 3]   [1, 64, 32, 32]   
6_layer1.0.BatchNorm2d_bn2                       [64]   [1, 64, 32, 32]   
7_layer1.0.Sequential_shortcut                      -   [1, 64, 32, 32]   
8_layer1.0.ReLU_relu2                               -   [1, 64, 32, 32]   
9_layer1.1.Conv2d_conv1                [64, 64, 3, 3]   [1, 64, 32, 32]   
10_layer1.1.BatchNorm2d_bn1                      [64]   [1, 64, 32, 32]   
11_layer1.1.ReLU_relu1                              -   [1, 64, 32, 32]   
12_layer1.1.Conv2d_conv2               [64, 64, 3, 3]   [1, 64, 32, 32]   
13_layer1.1.BatchNorm2d_bn2                      [64]   [1, 64, 32, 32]   
14_layer1.1.Sequential_shortcut                     -   [1, 64, 32, 32]   
15_layer1.1.ReLU_relu2                              -   [1, 64, 32, 32]   
16_layer1.2.Conv2d_conv1               [64, 64, 3, 3]   [1, 64, 32, 32]   
17_layer1.2.BatchNorm2d_bn1                      [64]   [1, 64, 32, 32]   
18_layer1.2.ReLU_relu1                              -   [1, 64, 32, 32]   
19_layer1.2.Conv2d_conv2               [64, 64, 3, 3]   [1, 64, 32, 32]   
20_layer1.2.BatchNorm2d_bn2                      [64]   [1, 64, 32, 32]   
21_layer1.2.Sequential_shortcut                     -   [1, 64, 32, 32]   
22_layer1.2.ReLU_relu2                              -   [1, 64, 32, 32]   
23_layer2.0.Conv2d_conv1              [64, 128, 3, 3]  [1, 128, 16, 16]   
24_layer2.0.BatchNorm2d_bn1                     [128]  [1, 128, 16, 16]   
25_layer2.0.ReLU_relu1                              -  [1, 128, 16, 16]   
26_layer2.0.Conv2d_conv2             [128, 128, 3, 3]  [1, 128, 16, 16]   
27_layer2.0.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
28_layer2.0.shortcut.Conv2d_0         [64, 128, 1, 1]  [1, 128, 16, 16]   
29_layer2.0.shortcut.BatchNorm2d_1              [128]  [1, 128, 16, 16]   
30_layer2.0.ReLU_relu2                              -  [1, 128, 16, 16]   
31_layer2.1.Conv2d_conv1             [128, 128, 3, 3]  [1, 128, 16, 16]   
32_layer2.1.BatchNorm2d_bn1                     [128]  [1, 128, 16, 16]   
33_layer2.1.ReLU_relu1                              -  [1, 128, 16, 16]   
34_layer2.1.Conv2d_conv2             [128, 128, 3, 3]  [1, 128, 16, 16]   
35_layer2.1.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
36_layer2.1.Sequential_shortcut                     -  [1, 128, 16, 16]   
37_layer2.1.ReLU_relu2                              -  [1, 128, 16, 16]   
38_layer2.2.Conv2d_conv1             [128, 128, 3, 3]  [1, 128, 16, 16]   
39_layer2.2.BatchNorm2d_bn1                     [128]  [1, 128, 16, 16]   
40_layer2.2.ReLU_relu1                              -  [1, 128, 16, 16]   
41_layer2.2.Conv2d_conv2             [128, 128, 3, 3]  [1, 128, 16, 16]   
42_layer2.2.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
43_layer2.2.Sequential_shortcut                     -  [1, 128, 16, 16]   
44_layer2.2.ReLU_relu2                              -  [1, 128, 16, 16]   
45_layer2.3.Conv2d_conv1             [128, 128, 3, 3]  [1, 128, 16, 16]   
46_layer2.3.BatchNorm2d_bn1                     [128]  [1, 128, 16, 16]   
47_layer2.3.ReLU_relu1                              -  [1, 128, 16, 16]   
48_layer2.3.Conv2d_conv2             [128, 128, 3, 3]  [1, 128, 16, 16]   
49_layer2.3.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
50_layer2.3.Sequential_shortcut                     -  [1, 128, 16, 16]   
51_layer2.3.ReLU_relu2                              -  [1, 128, 16, 16]   
52_layer3.0.Conv2d_conv1             [128, 256, 3, 3]    [1, 256, 8, 8]   
53_layer3.0.BatchNorm2d_bn1                     [256]    [1, 256, 8, 8]   
54_layer3.0.ReLU_relu1                              -    [1, 256, 8, 8]   
55_layer3.0.Conv2d_conv2             [256, 256, 3, 3]    [1, 256, 8, 8]   
56_layer3.0.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
57_layer3.0.shortcut.Conv2d_0        [128, 256, 1, 1]    [1, 256, 8, 8]   
58_layer3.0.shortcut.BatchNorm2d_1              [256]    [1, 256, 8, 8]   
59_layer3.0.ReLU_relu2                              -    [1, 256, 8, 8]   
60_layer3.1.Conv2d_conv1             [256, 256, 3, 3]    [1, 256, 8, 8]   
61_layer3.1.BatchNorm2d_bn1                     [256]    [1, 256, 8, 8]   
62_layer3.1.ReLU_relu1                              -    [1, 256, 8, 8]   
63_layer3.1.Conv2d_conv2             [256, 256, 3, 3]    [1, 256, 8, 8]   
64_layer3.1.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
65_layer3.1.Sequential_shortcut                     -    [1, 256, 8, 8]   
66_layer3.1.ReLU_relu2                              -    [1, 256, 8, 8]   
67_layer3.2.Conv2d_conv1             [256, 256, 3, 3]    [1, 256, 8, 8]   
68_layer3.2.BatchNorm2d_bn1                     [256]    [1, 256, 8, 8]   
69_layer3.2.ReLU_relu1                              -    [1, 256, 8, 8]   
70_layer3.2.Conv2d_conv2             [256, 256, 3, 3]    [1, 256, 8, 8]   
71_layer3.2.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
72_layer3.2.Sequential_shortcut                     -    [1, 256, 8, 8]   
73_layer3.2.ReLU_relu2                              -    [1, 256, 8, 8]   
74_layer3.3.Conv2d_conv1             [256, 256, 3, 3]    [1, 256, 8, 8]   
75_layer3.3.BatchNorm2d_bn1                     [256]    [1, 256, 8, 8]   
76_layer3.3.ReLU_relu1                              -    [1, 256, 8, 8]   
77_layer3.3.Conv2d_conv2             [256, 256, 3, 3]    [1, 256, 8, 8]   
78_layer3.3.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
79_layer3.3.Sequential_shortcut                     -    [1, 256, 8, 8]   
80_layer3.3.ReLU_relu2                              -    [1, 256, 8, 8]   
81_layer3.4.Conv2d_conv1             [256, 256, 3, 3]    [1, 256, 8, 8]   
82_layer3.4.BatchNorm2d_bn1                     [256]    [1, 256, 8, 8]   
83_layer3.4.ReLU_relu1                              -    [1, 256, 8, 8]   
84_layer3.4.Conv2d_conv2             [256, 256, 3, 3]    [1, 256, 8, 8]   
85_layer3.4.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
86_layer3.4.Sequential_shortcut                     -    [1, 256, 8, 8]   
87_layer3.4.ReLU_relu2                              -    [1, 256, 8, 8]   
88_layer3.5.Conv2d_conv1             [256, 256, 3, 3]    [1, 256, 8, 8]   
89_layer3.5.BatchNorm2d_bn1                     [256]    [1, 256, 8, 8]   
90_layer3.5.ReLU_relu1                              -    [1, 256, 8, 8]   
91_layer3.5.Conv2d_conv2             [256, 256, 3, 3]    [1, 256, 8, 8]   
92_layer3.5.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
93_layer3.5.Sequential_shortcut                     -    [1, 256, 8, 8]   
94_layer3.5.ReLU_relu2                              -    [1, 256, 8, 8]   
95_layer4.0.Conv2d_conv1             [256, 512, 3, 3]    [1, 512, 4, 4]   
96_layer4.0.BatchNorm2d_bn1                     [512]    [1, 512, 4, 4]   
97_layer4.0.ReLU_relu1                              -    [1, 512, 4, 4]   
98_layer4.0.Conv2d_conv2             [512, 512, 3, 3]    [1, 512, 4, 4]   
99_layer4.0.BatchNorm2d_bn2                     [512]    [1, 512, 4, 4]   
100_layer4.0.shortcut.Conv2d_0       [256, 512, 1, 1]    [1, 512, 4, 4]   
101_layer4.0.shortcut.BatchNorm2d_1             [512]    [1, 512, 4, 4]   
102_layer4.0.ReLU_relu2                             -    [1, 512, 4, 4]   
103_layer4.1.Conv2d_conv1            [512, 512, 3, 3]    [1, 512, 4, 4]   
104_layer4.1.BatchNorm2d_bn1                    [512]    [1, 512, 4, 4]   
105_layer4.1.ReLU_relu1                             -    [1, 512, 4, 4]   
106_layer4.1.Conv2d_conv2            [512, 512, 3, 3]    [1, 512, 4, 4]   
107_layer4.1.BatchNorm2d_bn2                    [512]    [1, 512, 4, 4]   
108_layer4.1.Sequential_shortcut                    -    [1, 512, 4, 4]   
109_layer4.1.ReLU_relu2                             -    [1, 512, 4, 4]   
110_layer4.2.Conv2d_conv1            [512, 512, 3, 3]    [1, 512, 4, 4]   
111_layer4.2.BatchNorm2d_bn1                    [512]    [1, 512, 4, 4]   
112_layer4.2.ReLU_relu1                             -    [1, 512, 4, 4]   
113_layer4.2.Conv2d_conv2            [512, 512, 3, 3]    [1, 512, 4, 4]   
114_layer4.2.BatchNorm2d_bn2                    [512]    [1, 512, 4, 4]   
115_layer4.2.Sequential_shortcut                    -    [1, 512, 4, 4]   
116_layer4.2.ReLU_relu2                             -    [1, 512, 4, 4]   
117_avgpool                                         -    [1, 512, 1, 1]   
118_linear                                 [512, 100]          [1, 100]   

                                        Params   Mult-Adds  
Layer                                                       
0_conv1                                 1.728k   1.769472M  
1_bn1                                    128.0        64.0  
2_layer1.0.Conv2d_conv1                36.864k  37.748736M  
3_layer1.0.BatchNorm2d_bn1               128.0        64.0  
4_layer1.0.ReLU_relu1                        -           -  
5_layer1.0.Conv2d_conv2                36.864k  37.748736M  
6_layer1.0.BatchNorm2d_bn2               128.0        64.0  
7_layer1.0.Sequential_shortcut               -           -  
8_layer1.0.ReLU_relu2                        -           -  
9_layer1.1.Conv2d_conv1                36.864k  37.748736M  
10_layer1.1.BatchNorm2d_bn1              128.0        64.0  
11_layer1.1.ReLU_relu1                       -           -  
12_layer1.1.Conv2d_conv2               36.864k  37.748736M  
13_layer1.1.BatchNorm2d_bn2              128.0        64.0  
14_layer1.1.Sequential_shortcut              -           -  
15_layer1.1.ReLU_relu2                       -           -  
16_layer1.2.Conv2d_conv1               36.864k  37.748736M  
17_layer1.2.BatchNorm2d_bn1              128.0        64.0  
18_layer1.2.ReLU_relu1                       -           -  
19_layer1.2.Conv2d_conv2               36.864k  37.748736M  
20_layer1.2.BatchNorm2d_bn2              128.0        64.0  
21_layer1.2.Sequential_shortcut              -           -  
22_layer1.2.ReLU_relu2                       -           -  
23_layer2.0.Conv2d_conv1               73.728k  18.874368M  
24_layer2.0.BatchNorm2d_bn1              256.0       128.0  
25_layer2.0.ReLU_relu1                       -           -  
26_layer2.0.Conv2d_conv2              147.456k  37.748736M  
27_layer2.0.BatchNorm2d_bn2              256.0       128.0  
28_layer2.0.shortcut.Conv2d_0           8.192k   2.097152M  
29_layer2.0.shortcut.BatchNorm2d_1       256.0       128.0  
30_layer2.0.ReLU_relu2                       -           -  
31_layer2.1.Conv2d_conv1              147.456k  37.748736M  
32_layer2.1.BatchNorm2d_bn1              256.0       128.0  
33_layer2.1.ReLU_relu1                       -           -  
34_layer2.1.Conv2d_conv2              147.456k  37.748736M  
35_layer2.1.BatchNorm2d_bn2              256.0       128.0  
36_layer2.1.Sequential_shortcut              -           -  
37_layer2.1.ReLU_relu2                       -           -  
38_layer2.2.Conv2d_conv1              147.456k  37.748736M  
39_layer2.2.BatchNorm2d_bn1              256.0       128.0  
40_layer2.2.ReLU_relu1                       -           -  
41_layer2.2.Conv2d_conv2              147.456k  37.748736M  
42_layer2.2.BatchNorm2d_bn2              256.0       128.0  
43_layer2.2.Sequential_shortcut              -           -  
44_layer2.2.ReLU_relu2                       -           -  
45_layer2.3.Conv2d_conv1              147.456k  37.748736M  
46_layer2.3.BatchNorm2d_bn1              256.0       128.0  
47_layer2.3.ReLU_relu1                       -           -  
48_layer2.3.Conv2d_conv2              147.456k  37.748736M  
49_layer2.3.BatchNorm2d_bn2              256.0       128.0  
50_layer2.3.Sequential_shortcut              -           -  
51_layer2.3.ReLU_relu2                       -           -  
52_layer3.0.Conv2d_conv1              294.912k  18.874368M  
53_layer3.0.BatchNorm2d_bn1              512.0       256.0  
54_layer3.0.ReLU_relu1                       -           -  
55_layer3.0.Conv2d_conv2              589.824k  37.748736M  
56_layer3.0.BatchNorm2d_bn2              512.0       256.0  
57_layer3.0.shortcut.Conv2d_0          32.768k   2.097152M  
58_layer3.0.shortcut.BatchNorm2d_1       512.0       256.0  
59_layer3.0.ReLU_relu2                       -           -  
60_layer3.1.Conv2d_conv1              589.824k  37.748736M  
61_layer3.1.BatchNorm2d_bn1              512.0       256.0  
62_layer3.1.ReLU_relu1                       -           -  
63_layer3.1.Conv2d_conv2              589.824k  37.748736M  
64_layer3.1.BatchNorm2d_bn2              512.0       256.0  
65_layer3.1.Sequential_shortcut              -           -  
66_layer3.1.ReLU_relu2                       -           -  
67_layer3.2.Conv2d_conv1              589.824k  37.748736M  
68_layer3.2.BatchNorm2d_bn1              512.0       256.0  
69_layer3.2.ReLU_relu1                       -           -  
70_layer3.2.Conv2d_conv2              589.824k  37.748736M  
71_layer3.2.BatchNorm2d_bn2              512.0       256.0  
72_layer3.2.Sequential_shortcut              -           -  
73_layer3.2.ReLU_relu2                       -           -  
74_layer3.3.Conv2d_conv1              589.824k  37.748736M  
75_layer3.3.BatchNorm2d_bn1              512.0       256.0  
76_layer3.3.ReLU_relu1                       -           -  
77_layer3.3.Conv2d_conv2              589.824k  37.748736M  
78_layer3.3.BatchNorm2d_bn2              512.0       256.0  
79_layer3.3.Sequential_shortcut              -           -  
80_layer3.3.ReLU_relu2                       -           -  
81_layer3.4.Conv2d_conv1              589.824k  37.748736M  
82_layer3.4.BatchNorm2d_bn1              512.0       256.0  
83_layer3.4.ReLU_relu1                       -           -  
84_layer3.4.Conv2d_conv2              589.824k  37.748736M  
85_layer3.4.BatchNorm2d_bn2              512.0       256.0  
86_layer3.4.Sequential_shortcut              -           -  
87_layer3.4.ReLU_relu2                       -           -  
88_layer3.5.Conv2d_conv1              589.824k  37.748736M  
89_layer3.5.BatchNorm2d_bn1              512.0       256.0  
90_layer3.5.ReLU_relu1                       -           -  
91_layer3.5.Conv2d_conv2              589.824k  37.748736M  
92_layer3.5.BatchNorm2d_bn2              512.0       256.0  
93_layer3.5.Sequential_shortcut              -           -  
94_layer3.5.ReLU_relu2                       -           -  
95_layer4.0.Conv2d_conv1             1.179648M  18.874368M  
96_layer4.0.BatchNorm2d_bn1             1.024k       512.0  
97_layer4.0.ReLU_relu1                       -           -  
98_layer4.0.Conv2d_conv2             2.359296M  37.748736M  
99_layer4.0.BatchNorm2d_bn2             1.024k       512.0  
100_layer4.0.shortcut.Conv2d_0        131.072k   2.097152M  
101_layer4.0.shortcut.BatchNorm2d_1     1.024k       512.0  
102_layer4.0.ReLU_relu2                      -           -  
103_layer4.1.Conv2d_conv1            2.359296M  37.748736M  
104_layer4.1.BatchNorm2d_bn1            1.024k       512.0  
105_layer4.1.ReLU_relu1                      -           -  
106_layer4.1.Conv2d_conv2            2.359296M  37.748736M  
107_layer4.1.BatchNorm2d_bn2            1.024k       512.0  
108_layer4.1.Sequential_shortcut             -           -  
109_layer4.1.ReLU_relu2                      -           -  
110_layer4.2.Conv2d_conv1            2.359296M  37.748736M  
111_layer4.2.BatchNorm2d_bn1            1.024k       512.0  
112_layer4.2.ReLU_relu1                      -           -  
113_layer4.2.Conv2d_conv2            2.359296M  37.748736M  
114_layer4.2.BatchNorm2d_bn2            1.024k       512.0  
115_layer4.2.Sequential_shortcut             -           -  
116_layer4.2.ReLU_relu2                      -           -  
117_avgpool                                  -           -  
118_linear                               51.3k       51.2k  
----------------------------------------------------------------------------------------------
                            Totals
Total params            21.328292M
Trainable params        21.328292M
Non-trainable params           0.0
Mult-Adds             1.159457088G
==============================================================================================
DATALOADER LABELS:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0.])
mmmmmmmmmmmmmmmmmmmmmmmmmm max_output mmmmmmmmmmmmmmmmmmmmmmmmmm
[[], [], [tensor([48, 40, 49, 30, 21, 23,  2, 34,  7, 16, 60, 63, 36, 12, 47, 58,  5, 61,
        51, 37, 10, 38, 28, 24, 33, 53, 54, 15, 62, 35,  9], device='cuda:0')], [tensor([18, 56, 52,  9, 47, 19, 60, 30, 40, 16,  2, 45, 15, 35, 13, 31, 53,  7,
         1, 33, 27, 44, 43, 37,  6, 41, 62, 39, 49,  4, 54], device='cuda:0')], [tensor([25, 50, 63, 58, 44, 13, 23, 36, 62, 60, 49, 24, 48,  7, 52,  2, 10, 51,
        39, 41,  6, 35, 61,  0, 32,  1, 26, 28, 14, 53, 54], device='cuda:0')], [tensor([104,   1,  74,  95,  70, 107,  78,  89, 117,  68,  91,  14, 119,  22,
         85,  18,  83,  45,  57,   7,  56,  98,  24, 110,  87, 125,  33, 103,
        126,  26, 101,  62, 120,  94, 111, 115,  75,  38,  10, 124,  97,  63,
         35,  99,   9, 112, 102,  30,  84, 108,  32,  15,   2,  17,  49,  77,
        121, 127,  42,   4,  64,   5], device='cuda:0')], [tensor([  1,  93,  51, 106,  29,  79, 121,  92, 104,  98,  59,  76,  99,   6,
         67,  66,  15,  33,   2,  37,  83,  80,  52,  96,  13,  17,  73, 112,
         78,  39,  86,  25,  40,  20, 100,  63, 126,  95,  68, 103, 125, 117,
         23,  11,  49,  36,  84,   3,  27,  65,  28, 120,  53,  56,  54,  61,
          8,  81,  77,  62,  69, 123], device='cuda:0')], [tensor([125,  28,  73,  15, 113,  56,  64, 107,  60,  91,  32,  83, 116,  13,
         39,  58,  75,  46,  31,   0,  61,  81,  85, 104,   5,  96, 122, 110,
         43,  19,  27,  50,  99,  65,  76, 127, 117,  79,  21,  88,  92,   3,
         17,  22,  82,   6,  25,  48, 114,  29,  14, 101, 111,   9,  11,   4,
        108,  35,  51,  57,  49, 100], device='cuda:0')], [tensor([  5,  94,  55,  34, 115,  88, 103,  72,  60, 110,  36,  92,  32,  53,
          6,   1,  63,  24,  96,  98,  90,  50, 123,  47,  69,  13, 108,  78,
         11,  49,  26,  65, 104,  56, 120,  51, 113,  70,   7,  99, 111,  19,
         41,  64,  73, 105,  45,  31,  79,  57,  62,   0,  48,  18,   8,  58,
        119,  15,  84,  67, 121,  42], device='cuda:0')], [tensor([ 57, 233, 175, 187,  64, 127, 112, 159, 122,  20,  15, 113,  17, 103,
        138,  32,  29, 231, 107, 148,  10, 253, 151, 182, 114,  66, 136, 223,
        100,  31,  80,  34,  65,   8,  85,  37, 174,  61, 104,  14, 219,  33,
        216,   0, 160, 185,  79,  41,  36,  28, 144, 209, 102, 192,  54,  77,
         82, 246, 238, 140,  30, 120,  93,  75, 162, 109, 221, 180,  87,  86,
        206,  89, 214, 198,  52, 163, 225, 224,  88,   9, 195, 154, 121, 244,
         27, 167, 243, 236,  48,  58, 237, 158,  91, 249,  40, 101, 155, 211,
        143,   4, 126, 232,  38, 106, 164, 194, 210,  76, 135, 254,  51, 217,
        229, 197, 248,  81,  12,  63, 139,  84,   7,  74, 212, 171,   2],
       device='cuda:0')], [tensor([170,  41,   3,  67, 180, 247,  14,  89,  13, 129,  11,  12, 150,  53,
        123, 162, 189,  71, 188, 250, 115, 131,  47, 178, 154,  15,  68,  69,
         80,  36, 240, 137, 181,  42, 212, 132, 236,  70, 168,   4, 116, 221,
          1, 143,   2,   7, 183, 210, 152, 200, 151, 121, 140, 112,  23, 107,
         29, 237, 224, 110,  39, 251, 160, 242, 118, 182, 145, 165,  30, 179,
        164, 198, 105,  32,  35,  58, 161,  62, 171, 141, 219, 214, 146, 202,
        199, 203,  99,  27,  33, 222,   5,  91,  57, 159,  97, 149, 144, 103,
        175,  87, 197,  46, 155, 217, 101, 233,  43,  82,  84, 252,  20, 126,
        134, 109, 117,  51, 241, 130, 248,  49, 125,  94,  60, 106,  17],
       device='cuda:0')], [tensor([ 27, 114,  70,  71,   1, 226, 112, 193,  19, 203, 184,  79, 210,  29,
        121,  41, 224, 111,  22, 127, 152,  64, 147, 126, 206, 221, 205, 181,
        108,  47, 168,   0,  88, 219, 190, 192, 139, 132, 196,  21, 100, 148,
        151,  54,  20, 161,  61,  34,  78, 174, 143,  16,  55,  48, 191,  87,
        236, 125, 172, 223,  92,   5, 122, 133, 156, 167,  95,  11, 220, 158,
         46, 107, 231, 252, 106,  59,  74,  90,  42, 243, 164,  37, 101,  10,
        175, 188,   6,  23, 189, 138,  93,  77, 173,   4, 136, 179, 199, 170,
        232, 213, 255,  85, 146, 160, 145, 185, 217, 116, 216, 240,  75, 200,
        162, 131,  44,  56,  67,  98, 157, 166, 110,  83, 141,  73,  43],
       device='cuda:0')], [tensor([ 70, 210, 200,  19,  61, 127, 216, 169, 253,  62,  95,  39, 113, 110,
         63,  99, 235, 167,  24, 239,  68, 205,  82,  13,  35, 102, 114, 168,
        165,  27, 133, 221, 172, 231, 155, 208, 104, 207,  98,  40,  72,  15,
         32, 137,  79, 193, 255, 251,   3, 105, 240, 129,  85, 136,   1, 166,
         78, 252, 139,   4,  69,   9, 191, 157, 242,   5, 143, 217,  59,  93,
        254, 160,  67,  89, 121, 192,   6,   2, 111, 112, 122, 227, 107,  17,
        199, 224,  29,  25,  83, 246,  76, 103,  42,  90, 189, 202, 238,   7,
        225, 152,   8, 249, 148,  92, 195,  73, 163,  31, 248, 126, 125,  34,
        146, 159, 150, 244,  18, 158, 184, 229,  66, 141,  53, 215, 164],
       device='cuda:0')], [tensor([123, 152, 149,  11,  38, 188, 151, 240,  90, 217,  21,  63, 228, 135,
         58, 132, 206, 201, 179,  85, 178, 100, 234, 122,  54, 204, 174, 222,
         44, 115, 141, 223,  35, 210, 187, 253, 137, 121, 231, 134, 193, 162,
        106,  96, 183,  64,  91,  36, 138,  67,  41,  66, 221,  95,  19,   2,
         81, 255,  62,  74, 214, 191, 169,  59, 140,  24, 173,  42, 230, 144,
        242, 236, 104, 238, 224,  89,  79, 250, 126,  87, 120, 101, 235,  49,
        139, 215, 160, 163,  33,  37, 177,  82,  22, 227,   9, 203,  14, 107,
        148, 205, 243, 213,  10, 225,  34, 247, 165,  32, 158, 196, 229,  99,
        200, 218, 111,  98,   0,   7, 164, 220, 105,  23,  18,  51,  70],
       device='cuda:0')], [tensor([112, 194, 186, 203,  87,  10, 145,  50, 173,  31,  40, 182, 215, 246,
        163,  32,  29,  54, 126,  69,  42, 101, 135, 207,  79, 223,   7, 105,
        170, 192, 169, 117,  73, 174, 162, 191, 154,  77, 139, 229, 143, 250,
        248,   5,  23,  46,  93,  72,  91,  71,  83, 184, 197,  96,  25,  86,
        187,  66,   9, 104, 115, 167, 199, 178, 147, 211, 103, 127, 120,  43,
         28, 227,  89,  85, 247,  63,  68, 202, 189, 109,  80, 102,  57, 100,
         99,  67,  70,  37,  52, 181,  94, 137, 159, 124,  59,  65,  20, 111,
         92,   2,  56,  51,  26, 221, 151, 209, 224, 166,  75, 212,  13,  17,
        171,  61, 140,  33,  88,  49, 200, 238, 130, 153,  76, 213, 180],
       device='cuda:0')], [tensor([489, 155, 298, 507, 293, 473,  17, 455, 482, 132, 362, 123, 161, 371,
        326, 256,  37, 253, 438, 215,  68, 406, 463, 417, 178,  84, 483, 395,
        351,  80,   7, 243, 360, 108, 160, 204, 227, 343,  57, 258, 156,   1,
        323,  50, 321, 361,  77, 146, 224, 281, 296, 129, 141,  40, 314, 195,
        117, 181, 393, 101, 267, 143, 284, 335, 128, 354, 409, 230, 177, 122,
        131, 294, 376,  45, 250, 169,   6, 337, 172,  33,  85, 192, 222,  15,
         23, 304, 171, 121, 147, 242, 357,  81, 501, 341, 446,  67,  42, 340,
        190, 213, 275, 465, 345, 249, 175, 426, 436, 504, 414, 188, 295, 468,
         65, 174, 368, 185, 319, 312, 150, 300, 460, 342, 206, 276,  63, 452,
        478, 382, 202, 272, 355, 334, 374, 299, 428,  61, 394, 180, 392, 401,
        103, 240, 347, 183, 127,  30, 107, 367, 491, 310, 503, 125, 408,  90,
        157,  48, 279, 307, 241, 411, 493, 134,  71, 464, 346, 187, 332, 472,
        477, 377, 287, 111, 449, 232,  78, 154, 316, 431, 469,  14, 425, 283,
         46,  94, 274, 442, 261,  75, 510, 430, 166,  95, 508, 485, 288, 492,
        136,  91, 118, 110,  25, 331,  59, 100, 109, 303, 282, 217,  74, 349,
        207,  22,  34,  26, 142,  19, 239,  35, 415, 163, 358, 176, 105, 292,
        412, 189, 149, 291,  36,  41, 179, 104, 306,  79, 220, 405,  98, 344,
        277, 348,  49, 328, 290, 329, 116, 212, 130, 475, 152, 268],
       device='cuda:0')], [tensor([174, 373, 420, 232, 334, 305, 150, 336,  88,  92, 370, 129, 134, 331,
        163, 175, 196, 454, 295, 369, 417, 161, 498, 243, 208, 372, 500,  46,
        394, 346, 210, 466, 426, 358,  59, 389, 253, 328, 416,   9, 509, 382,
        396, 410, 217, 367, 116, 414, 121, 442,  62, 451, 206, 235, 287, 301,
        423, 440, 505, 383,  84, 205,  44,  86, 279, 438, 153,  79, 352, 380,
         76, 325, 200,  30, 411, 491, 184, 326, 286, 354, 236,  54,  50,  96,
        139, 182, 231, 254,  32,  80, 384, 141, 212, 288, 283,  28, 229, 452,
        397, 294, 323, 221, 441,  43, 298,   3, 437, 154, 393, 377, 398,  67,
        408,  36, 395,  75, 275, 297, 113, 114, 413, 101, 424, 123,  31, 187,
        474, 502, 460, 485, 387, 258,  89, 313,  48, 360, 242, 162, 307,  13,
         11,  10, 362, 115,   6,   1,  58, 473, 157,  27, 251, 378, 237, 223,
         77, 347,  26, 497,  78, 156, 489, 501, 340, 285, 388, 105,   2, 335,
        266, 390, 167, 391, 299, 386, 256, 257, 151, 316, 102, 345,  82, 219,
        433, 486, 376, 511,  95, 342, 462, 106, 247, 216, 169,  91,  93, 478,
        343,  72, 100,  35, 292, 468, 371, 445, 291,  81, 274, 270, 439, 249,
        349, 429, 310, 224, 499,  24, 443, 191, 197, 268, 276, 220,  45, 385,
        317, 230, 332, 190, 126, 444,  97, 483, 104, 209, 322,   8, 108, 132,
        404,  33, 406, 250, 158, 374, 183,  64,  39, 484, 215, 412],
       device='cuda:0')], [tensor([ 59, 462, 477, 334,   1, 210, 175, 309, 281,  29,  40, 271, 509, 103,
         96, 361, 398, 204, 289, 401, 186, 484, 510, 412, 333,  22, 389, 216,
        294, 232, 463, 157, 379,  68,  23, 437, 353, 165,  84,  14, 488,  92,
          2, 156, 215, 479, 127, 363,   0, 416,  74, 194, 262,  52, 322,  56,
         97,  47, 117, 339, 302, 503,  36, 377,  54, 134, 482, 116,  38,  88,
        207, 178, 240, 225, 395, 264, 445, 430, 431, 483, 140,  61, 261, 260,
         21,  50, 394, 415, 130,  99, 252, 220, 275, 241, 170, 222, 499, 258,
         89, 347, 292, 413, 231,  60, 144, 335, 120, 213, 164, 270, 494, 107,
         43, 508, 131, 168, 257, 110, 313, 126, 211, 383, 196,  67, 182, 414,
         94,  78, 450, 276,  17,  37, 167,  65,  90, 191, 218, 350, 265, 342,
        443, 376, 471, 267, 205, 424, 419, 384, 436,  48, 203, 145, 247, 256,
        438,  16, 341, 461, 242, 198,  46, 129, 100, 356, 106, 366,  30, 163,
        318, 269, 321, 370,  85, 251, 344, 358,  93,  98, 185,  91, 466,  83,
        452, 385, 340,  77, 192, 161, 286, 457, 290,  70,   3, 219, 372,   5,
        238, 489, 143, 310, 248,  95, 200, 297, 226, 195, 359, 470,  20, 425,
        244,  53, 502, 402, 279, 504, 174, 423, 206,  11, 142, 217,  79, 284,
        230, 149, 317, 409, 237, 380, 153, 146, 305, 132, 399,  55, 400, 223,
        332, 493, 312, 451, 254, 454,  45, 417, 135, 329, 151, 224],
       device='cuda:0')], [], []]
==============================================================================================
                                         Kernel Shape      Output Shape  \
Layer                                                                     
0_conv1                                 [3, 64, 3, 3]   [1, 64, 32, 32]   
1_bn1                                            [64]   [1, 64, 32, 32]   
2_layer1.0.Conv2d_conv1                [64, 31, 3, 3]   [1, 31, 32, 32]   
3_layer1.0.BatchNorm2d_bn1                       [31]   [1, 31, 32, 32]   
4_layer1.0.ReLU_relu1                               -   [1, 31, 32, 32]   
5_layer1.0.Conv2d_conv2                [31, 64, 3, 3]   [1, 64, 32, 32]   
6_layer1.0.BatchNorm2d_bn2                       [64]   [1, 64, 32, 32]   
7_layer1.0.Sequential_shortcut                      -   [1, 64, 32, 32]   
8_layer1.0.ReLU_relu2                               -   [1, 64, 32, 32]   
9_layer1.1.Conv2d_conv1                [64, 31, 3, 3]   [1, 31, 32, 32]   
10_layer1.1.BatchNorm2d_bn1                      [31]   [1, 31, 32, 32]   
11_layer1.1.ReLU_relu1                              -   [1, 31, 32, 32]   
12_layer1.1.Conv2d_conv2               [31, 64, 3, 3]   [1, 64, 32, 32]   
13_layer1.1.BatchNorm2d_bn2                      [64]   [1, 64, 32, 32]   
14_layer1.1.Sequential_shortcut                     -   [1, 64, 32, 32]   
15_layer1.1.ReLU_relu2                              -   [1, 64, 32, 32]   
16_layer1.2.Conv2d_conv1               [64, 31, 3, 3]   [1, 31, 32, 32]   
17_layer1.2.BatchNorm2d_bn1                      [31]   [1, 31, 32, 32]   
18_layer1.2.ReLU_relu1                              -   [1, 31, 32, 32]   
19_layer1.2.Conv2d_conv2               [31, 64, 3, 3]   [1, 64, 32, 32]   
20_layer1.2.BatchNorm2d_bn2                      [64]   [1, 64, 32, 32]   
21_layer1.2.Sequential_shortcut                     -   [1, 64, 32, 32]   
22_layer1.2.ReLU_relu2                              -   [1, 64, 32, 32]   
23_layer2.0.Conv2d_conv1               [64, 62, 3, 3]   [1, 62, 16, 16]   
24_layer2.0.BatchNorm2d_bn1                      [62]   [1, 62, 16, 16]   
25_layer2.0.ReLU_relu1                              -   [1, 62, 16, 16]   
26_layer2.0.Conv2d_conv2              [62, 128, 3, 3]  [1, 128, 16, 16]   
27_layer2.0.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
28_layer2.0.shortcut.Conv2d_0         [64, 128, 1, 1]  [1, 128, 16, 16]   
29_layer2.0.shortcut.BatchNorm2d_1              [128]  [1, 128, 16, 16]   
30_layer2.0.ReLU_relu2                              -  [1, 128, 16, 16]   
31_layer2.1.Conv2d_conv1              [128, 62, 3, 3]   [1, 62, 16, 16]   
32_layer2.1.BatchNorm2d_bn1                      [62]   [1, 62, 16, 16]   
33_layer2.1.ReLU_relu1                              -   [1, 62, 16, 16]   
34_layer2.1.Conv2d_conv2              [62, 128, 3, 3]  [1, 128, 16, 16]   
35_layer2.1.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
36_layer2.1.Sequential_shortcut                     -  [1, 128, 16, 16]   
37_layer2.1.ReLU_relu2                              -  [1, 128, 16, 16]   
38_layer2.2.Conv2d_conv1              [128, 62, 3, 3]   [1, 62, 16, 16]   
39_layer2.2.BatchNorm2d_bn1                      [62]   [1, 62, 16, 16]   
40_layer2.2.ReLU_relu1                              -   [1, 62, 16, 16]   
41_layer2.2.Conv2d_conv2              [62, 128, 3, 3]  [1, 128, 16, 16]   
42_layer2.2.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
43_layer2.2.Sequential_shortcut                     -  [1, 128, 16, 16]   
44_layer2.2.ReLU_relu2                              -  [1, 128, 16, 16]   
45_layer2.3.Conv2d_conv1              [128, 62, 3, 3]   [1, 62, 16, 16]   
46_layer2.3.BatchNorm2d_bn1                      [62]   [1, 62, 16, 16]   
47_layer2.3.ReLU_relu1                              -   [1, 62, 16, 16]   
48_layer2.3.Conv2d_conv2              [62, 128, 3, 3]  [1, 128, 16, 16]   
49_layer2.3.BatchNorm2d_bn2                     [128]  [1, 128, 16, 16]   
50_layer2.3.Sequential_shortcut                     -  [1, 128, 16, 16]   
51_layer2.3.ReLU_relu2                              -  [1, 128, 16, 16]   
52_layer3.0.Conv2d_conv1             [128, 125, 3, 3]    [1, 125, 8, 8]   
53_layer3.0.BatchNorm2d_bn1                     [125]    [1, 125, 8, 8]   
54_layer3.0.ReLU_relu1                              -    [1, 125, 8, 8]   
55_layer3.0.Conv2d_conv2             [125, 256, 3, 3]    [1, 256, 8, 8]   
56_layer3.0.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
57_layer3.0.shortcut.Conv2d_0        [128, 256, 1, 1]    [1, 256, 8, 8]   
58_layer3.0.shortcut.BatchNorm2d_1              [256]    [1, 256, 8, 8]   
59_layer3.0.ReLU_relu2                              -    [1, 256, 8, 8]   
60_layer3.1.Conv2d_conv1             [256, 125, 3, 3]    [1, 125, 8, 8]   
61_layer3.1.BatchNorm2d_bn1                     [125]    [1, 125, 8, 8]   
62_layer3.1.ReLU_relu1                              -    [1, 125, 8, 8]   
63_layer3.1.Conv2d_conv2             [125, 256, 3, 3]    [1, 256, 8, 8]   
64_layer3.1.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
65_layer3.1.Sequential_shortcut                     -    [1, 256, 8, 8]   
66_layer3.1.ReLU_relu2                              -    [1, 256, 8, 8]   
67_layer3.2.Conv2d_conv1             [256, 125, 3, 3]    [1, 125, 8, 8]   
68_layer3.2.BatchNorm2d_bn1                     [125]    [1, 125, 8, 8]   
69_layer3.2.ReLU_relu1                              -    [1, 125, 8, 8]   
70_layer3.2.Conv2d_conv2             [125, 256, 3, 3]    [1, 256, 8, 8]   
71_layer3.2.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
72_layer3.2.Sequential_shortcut                     -    [1, 256, 8, 8]   
73_layer3.2.ReLU_relu2                              -    [1, 256, 8, 8]   
74_layer3.3.Conv2d_conv1             [256, 125, 3, 3]    [1, 125, 8, 8]   
75_layer3.3.BatchNorm2d_bn1                     [125]    [1, 125, 8, 8]   
76_layer3.3.ReLU_relu1                              -    [1, 125, 8, 8]   
77_layer3.3.Conv2d_conv2             [125, 256, 3, 3]    [1, 256, 8, 8]   
78_layer3.3.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
79_layer3.3.Sequential_shortcut                     -    [1, 256, 8, 8]   
80_layer3.3.ReLU_relu2                              -    [1, 256, 8, 8]   
81_layer3.4.Conv2d_conv1             [256, 125, 3, 3]    [1, 125, 8, 8]   
82_layer3.4.BatchNorm2d_bn1                     [125]    [1, 125, 8, 8]   
83_layer3.4.ReLU_relu1                              -    [1, 125, 8, 8]   
84_layer3.4.Conv2d_conv2             [125, 256, 3, 3]    [1, 256, 8, 8]   
85_layer3.4.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
86_layer3.4.Sequential_shortcut                     -    [1, 256, 8, 8]   
87_layer3.4.ReLU_relu2                              -    [1, 256, 8, 8]   
88_layer3.5.Conv2d_conv1             [256, 125, 3, 3]    [1, 125, 8, 8]   
89_layer3.5.BatchNorm2d_bn1                     [125]    [1, 125, 8, 8]   
90_layer3.5.ReLU_relu1                              -    [1, 125, 8, 8]   
91_layer3.5.Conv2d_conv2             [125, 256, 3, 3]    [1, 256, 8, 8]   
92_layer3.5.BatchNorm2d_bn2                     [256]    [1, 256, 8, 8]   
93_layer3.5.Sequential_shortcut                     -    [1, 256, 8, 8]   
94_layer3.5.ReLU_relu2                              -    [1, 256, 8, 8]   
95_layer4.0.Conv2d_conv1             [256, 250, 3, 3]    [1, 250, 4, 4]   
96_layer4.0.BatchNorm2d_bn1                     [250]    [1, 250, 4, 4]   
97_layer4.0.ReLU_relu1                              -    [1, 250, 4, 4]   
98_layer4.0.Conv2d_conv2             [250, 512, 3, 3]    [1, 512, 4, 4]   
99_layer4.0.BatchNorm2d_bn2                     [512]    [1, 512, 4, 4]   
100_layer4.0.shortcut.Conv2d_0       [256, 512, 1, 1]    [1, 512, 4, 4]   
101_layer4.0.shortcut.BatchNorm2d_1             [512]    [1, 512, 4, 4]   
102_layer4.0.ReLU_relu2                             -    [1, 512, 4, 4]   
103_layer4.1.Conv2d_conv1            [512, 250, 3, 3]    [1, 250, 4, 4]   
104_layer4.1.BatchNorm2d_bn1                    [250]    [1, 250, 4, 4]   
105_layer4.1.ReLU_relu1                             -    [1, 250, 4, 4]   
106_layer4.1.Conv2d_conv2            [250, 512, 3, 3]    [1, 512, 4, 4]   
107_layer4.1.BatchNorm2d_bn2                    [512]    [1, 512, 4, 4]   
108_layer4.1.Sequential_shortcut                    -    [1, 512, 4, 4]   
109_layer4.1.ReLU_relu2                             -    [1, 512, 4, 4]   
110_layer4.2.Conv2d_conv1            [512, 250, 3, 3]    [1, 250, 4, 4]   
111_layer4.2.BatchNorm2d_bn1                    [250]    [1, 250, 4, 4]   
112_layer4.2.ReLU_relu1                             -    [1, 250, 4, 4]   
113_layer4.2.Conv2d_conv2            [250, 512, 3, 3]    [1, 512, 4, 4]   
114_layer4.2.BatchNorm2d_bn2                    [512]    [1, 512, 4, 4]   
115_layer4.2.Sequential_shortcut                    -    [1, 512, 4, 4]   
116_layer4.2.ReLU_relu2                             -    [1, 512, 4, 4]   
117_avgpool                                         -    [1, 512, 1, 1]   
118_linear                                 [512, 100]          [1, 100]   

                                       Params   Mult-Adds  
Layer                                                      
0_conv1                                1.728k   1.769472M  
1_bn1                                   128.0        64.0  
2_layer1.0.Conv2d_conv1               17.856k  18.284544M  
3_layer1.0.BatchNorm2d_bn1               62.0        31.0  
4_layer1.0.ReLU_relu1                       -           -  
5_layer1.0.Conv2d_conv2               17.856k  18.284544M  
6_layer1.0.BatchNorm2d_bn2              128.0        64.0  
7_layer1.0.Sequential_shortcut              -           -  
8_layer1.0.ReLU_relu2                       -           -  
9_layer1.1.Conv2d_conv1               17.856k  18.284544M  
10_layer1.1.BatchNorm2d_bn1              62.0        31.0  
11_layer1.1.ReLU_relu1                      -           -  
12_layer1.1.Conv2d_conv2              17.856k  18.284544M  
13_layer1.1.BatchNorm2d_bn2             128.0        64.0  
14_layer1.1.Sequential_shortcut             -           -  
15_layer1.1.ReLU_relu2                      -           -  
16_layer1.2.Conv2d_conv1              17.856k  18.284544M  
17_layer1.2.BatchNorm2d_bn1              62.0        31.0  
18_layer1.2.ReLU_relu1                      -           -  
19_layer1.2.Conv2d_conv2              17.856k  18.284544M  
20_layer1.2.BatchNorm2d_bn2             128.0        64.0  
21_layer1.2.Sequential_shortcut             -           -  
22_layer1.2.ReLU_relu2                      -           -  
23_layer2.0.Conv2d_conv1              35.712k   9.142272M  
24_layer2.0.BatchNorm2d_bn1             124.0        62.0  
25_layer2.0.ReLU_relu1                      -           -  
26_layer2.0.Conv2d_conv2              71.424k  18.284544M  
27_layer2.0.BatchNorm2d_bn2             256.0       128.0  
28_layer2.0.shortcut.Conv2d_0          8.192k   2.097152M  
29_layer2.0.shortcut.BatchNorm2d_1      256.0       128.0  
30_layer2.0.ReLU_relu2                      -           -  
31_layer2.1.Conv2d_conv1              71.424k  18.284544M  
32_layer2.1.BatchNorm2d_bn1             124.0        62.0  
33_layer2.1.ReLU_relu1                      -           -  
34_layer2.1.Conv2d_conv2              71.424k  18.284544M  
35_layer2.1.BatchNorm2d_bn2             256.0       128.0  
36_layer2.1.Sequential_shortcut             -           -  
37_layer2.1.ReLU_relu2                      -           -  
38_layer2.2.Conv2d_conv1              71.424k  18.284544M  
39_layer2.2.BatchNorm2d_bn1             124.0        62.0  
40_layer2.2.ReLU_relu1                      -           -  
41_layer2.2.Conv2d_conv2              71.424k  18.284544M  
42_layer2.2.BatchNorm2d_bn2             256.0       128.0  
43_layer2.2.Sequential_shortcut             -           -  
44_layer2.2.ReLU_relu2                      -           -  
45_layer2.3.Conv2d_conv1              71.424k  18.284544M  
46_layer2.3.BatchNorm2d_bn1             124.0        62.0  
47_layer2.3.ReLU_relu1                      -           -  
48_layer2.3.Conv2d_conv2              71.424k  18.284544M  
49_layer2.3.BatchNorm2d_bn2             256.0       128.0  
50_layer2.3.Sequential_shortcut             -           -  
51_layer2.3.ReLU_relu2                      -           -  
52_layer3.0.Conv2d_conv1               144.0k      9.216M  
53_layer3.0.BatchNorm2d_bn1             250.0       125.0  
54_layer3.0.ReLU_relu1                      -           -  
55_layer3.0.Conv2d_conv2               288.0k     18.432M  
56_layer3.0.BatchNorm2d_bn2             512.0       256.0  
57_layer3.0.shortcut.Conv2d_0         32.768k   2.097152M  
58_layer3.0.shortcut.BatchNorm2d_1      512.0       256.0  
59_layer3.0.ReLU_relu2                      -           -  
60_layer3.1.Conv2d_conv1               288.0k     18.432M  
61_layer3.1.BatchNorm2d_bn1             250.0       125.0  
62_layer3.1.ReLU_relu1                      -           -  
63_layer3.1.Conv2d_conv2               288.0k     18.432M  
64_layer3.1.BatchNorm2d_bn2             512.0       256.0  
65_layer3.1.Sequential_shortcut             -           -  
66_layer3.1.ReLU_relu2                      -           -  
67_layer3.2.Conv2d_conv1               288.0k     18.432M  
68_layer3.2.BatchNorm2d_bn1             250.0       125.0  
69_layer3.2.ReLU_relu1                      -           -  
70_layer3.2.Conv2d_conv2               288.0k     18.432M  
71_layer3.2.BatchNorm2d_bn2             512.0       256.0  
72_layer3.2.Sequential_shortcut             -           -  
73_layer3.2.ReLU_relu2                      -           -  
74_layer3.3.Conv2d_conv1               288.0k     18.432M  
75_layer3.3.BatchNorm2d_bn1             250.0       125.0  
76_layer3.3.ReLU_relu1                      -           -  
77_layer3.3.Conv2d_conv2               288.0k     18.432M  
78_layer3.3.BatchNorm2d_bn2             512.0       256.0  
79_layer3.3.Sequential_shortcut             -           -  
80_layer3.3.ReLU_relu2                      -           -  
81_layer3.4.Conv2d_conv1               288.0k     18.432M  
82_layer3.4.BatchNorm2d_bn1             250.0       125.0  
83_layer3.4.ReLU_relu1                      -           -  
84_layer3.4.Conv2d_conv2               288.0k     18.432M  
85_layer3.4.BatchNorm2d_bn2             512.0       256.0  
86_layer3.4.Sequential_shortcut             -           -  
87_layer3.4.ReLU_relu2                      -           -  
88_layer3.5.Conv2d_conv1               288.0k     18.432M  
89_layer3.5.BatchNorm2d_bn1             250.0       125.0  
90_layer3.5.ReLU_relu1                      -           -  
91_layer3.5.Conv2d_conv2               288.0k     18.432M  
92_layer3.5.BatchNorm2d_bn2             512.0       256.0  
93_layer3.5.Sequential_shortcut             -           -  
94_layer3.5.ReLU_relu2                      -           -  
95_layer4.0.Conv2d_conv1               576.0k      9.216M  
96_layer4.0.BatchNorm2d_bn1             500.0       250.0  
97_layer4.0.ReLU_relu1                      -           -  
98_layer4.0.Conv2d_conv2               1.152M     18.432M  
99_layer4.0.BatchNorm2d_bn2            1.024k       512.0  
100_layer4.0.shortcut.Conv2d_0       131.072k   2.097152M  
101_layer4.0.shortcut.BatchNorm2d_1    1.024k       512.0  
102_layer4.0.ReLU_relu2                     -           -  
103_layer4.1.Conv2d_conv1              1.152M     18.432M  
104_layer4.1.BatchNorm2d_bn1            500.0       250.0  
105_layer4.1.ReLU_relu1                     -           -  
106_layer4.1.Conv2d_conv2              1.152M     18.432M  
107_layer4.1.BatchNorm2d_bn2           1.024k       512.0  
108_layer4.1.Sequential_shortcut            -           -  
109_layer4.1.ReLU_relu2                     -           -  
110_layer4.2.Conv2d_conv1              1.152M     18.432M  
111_layer4.2.BatchNorm2d_bn1            500.0       250.0  
112_layer4.2.ReLU_relu1                     -           -  
113_layer4.2.Conv2d_conv2              1.152M     18.432M  
114_layer4.2.BatchNorm2d_bn2           1.024k       512.0  
115_layer4.2.Sequential_shortcut            -           -  
116_layer4.2.ReLU_relu2                     -           -  
117_avgpool                                 -           -  
118_linear                              51.3k       51.2k  
----------------------------------------------------------------------------------------------
                           Totals
Total params            10.52903M
Trainable params        10.52903M
Non-trainable params          0.0
Mult-Adds             568.304049M
==============================================================================================
**[TEST] Loss 1.9481)	 Acc: 58.800)	


==========================================================
#                                                          #
#                  Test acc : 58.8000                  #
#                                                          #
==========================================================

[Epoch 5/30] [TRAIN] Loss 1.3440)	 Acc: 83.691)	 Time 0.432)
[Epoch 10/30] [TRAIN] Loss 0.5933)	 Acc: 89.897)	 Time 0.312)
**[Epoch 10/30] [VAL] Loss 0.4470)	 Acc: 94.153)	 TotalTime: 0h 1min

[Epoch 15/30] [TRAIN] Loss 0.3663)	 Acc: 92.323)	 Time 0.434)
[Epoch 20/30] [TRAIN] Loss 0.2549)	 Acc: 94.670)	 Time 0.453)
**[Epoch 20/30] [VAL] Loss 0.1812)	 Acc: 96.659)	 TotalTime: 0h 3min

[Epoch 25/30] [TRAIN] Loss 0.2042)	 Acc: 95.664)	 Time 0.312)
[Epoch 30/30] [TRAIN] Loss 0.1721)	 Acc: 96.340)	 Time 0.440)
**[Epoch 30/30] [VAL] Loss 0.1070)	 Acc: 98.170)	 TotalTime: 0h 4min

**[TEST] Loss 0.5495)	 Acc: 83.800)	


==========================================================
#                                                          #
#                  Test acc : 83.8000                  #
#                                                          #
==========================================================

